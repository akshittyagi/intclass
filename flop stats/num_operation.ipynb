{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torchstat\n",
    "\n",
    "from torchstat import stat\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(56180, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 56180)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class ThreeLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ThreeLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, output_dim)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('three:', x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class aLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(aLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.exit_1 = nn.Linear(100, output_dim)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.exit_2 = nn.Linear(50, output_dim)\n",
    "        self.fc3 = nn.Linear(50, output_dim)\n",
    "        self.exit_3 = nn.Linear(output_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        exit_1 = self.exit_1(x)\n",
    "        sm_1 = F.softmax(exit_1, dim=0)\n",
    "        neg_entropy_1 = torch.sum(sm_1 * torch.log(sm_1))\n",
    "        return exit_1\n",
    "        \n",
    "        \n",
    "    \n",
    "class abLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(abLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.exit_1 = nn.Linear(100, output_dim)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.exit_2 = nn.Linear(50, output_dim)\n",
    "        self.fc3 = nn.Linear(50, output_dim)\n",
    "        self.exit_3 = nn.Linear(output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        exit_1 = self.exit_1(x)\n",
    "        sm_1 = F.softmax(exit_1, dim=0)\n",
    "        neg_entropy_1 = torch.sum(sm_1 * torch.log(sm_1))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        exit_2 = self.exit_2(x)\n",
    "        sm_2 = F.softmax(exit_2, dim=0)\n",
    "        neg_entropy_2 = torch.sum(sm_2 * torch.log(sm_2))\n",
    "        return 2, exit_2\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        #self.batch_first = True\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "        self.num_layers = 2\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.d_emb, self.d_hid, self.num_layers, batch_first=True, dropout = 0.02)\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.W = nn.Linear(self.d_hid, self.vocab_size) \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "      \n",
    "        # each example in a batch is of the form <BOS> w1 w2 ... wn <EOS>\n",
    "        # we want to predict everything except the <BOS> tokens\n",
    "        bsz, seq_len = batch.size()\n",
    "        embs = self.embeddings(batch)\n",
    "        \n",
    "        out, _ = self.lstm(embs)\n",
    "        out = self.W(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ThreeLayerBN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ThreeLayerBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.exit_1 = nn.Linear(100, output_dim)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.exit_2 = nn.Linear(50, output_dim)\n",
    "        self.fc3 = nn.Linear(50, output_dim)\n",
    "        self.exit_3 = nn.Linear(output_dim, output_dim)\n",
    "   \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        exit_1 = self.exit_1(x)\n",
    "        sm_1 = F.softmax(exit_1, dim=0)\n",
    "        neg_entropy_1 = torch.sum(sm_1 * torch.log(sm_1))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        exit_2 = self.exit_2(x)\n",
    "        sm_2 = F.softmax(exit_2, dim=0)\n",
    "        neg_entropy_2 = torch.sum(sm_2 * torch.log(sm_2))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        exit_3 = self.exit_3(x)\n",
    "        return 3, exit_3\n",
    "\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, hidden_dim=1200, embedding_dim=300, num_layers=3):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.inp = nn.Linear(self.embedding_dim, self.hidden_dim)\n",
    "        self.rnns = [nn.LSTM(self.hidden_dim,\n",
    "                             self.hidden_dim, batch_first=True)\n",
    "                     for i in range(self.num_layers)]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.inp.weight)\n",
    "        nn.init.kaiming_normal_(self.out.weight)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inp = self.inp(batch)\n",
    "        lstm_out = inp\n",
    "\n",
    "        for i, layer in enumerate(self.rnns):\n",
    "            lstm_out, (h, c) = layer(lstm_out)\n",
    "\n",
    "        logits = self.out(h)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      module name input shape output shape   params memory(MB)      MAdd     Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)\n",
      "0             fc1         300          100  30100.0       0.00  59,900.0  30,000.0    121600.0        400.0      20.73%   122000.0\n",
      "1          exit_1         100           25   2525.0       0.00   4,975.0   2,500.0     10500.0        100.0      15.93%    10600.0\n",
      "2             fc2         100           50   5050.0       0.00   9,950.0   5,000.0     20600.0        200.0      17.23%    20800.0\n",
      "3          exit_2          50           25   1275.0       0.00   2,475.0   1,250.0      5300.0        100.0      15.98%     5400.0\n",
      "4             fc3          50           25   1275.0       0.00   2,475.0   1,250.0      5300.0        100.0      14.52%     5400.0\n",
      "5          exit_3          25           25    650.0       0.00   1,225.0     625.0      2700.0        100.0      15.60%     2800.0\n",
      "total                                       40875.0       0.00  81,000.0  40,625.0      2700.0        100.0     100.00%   167000.0\n",
      "==================================================================================================================================\n",
      "Total params: 40,875\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 0.00MB\n",
      "Total MAdd: 81.0KMAdd\n",
      "Total Flops: 40.62KFlops\n",
      "Total MemR+W: 163.09KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test size 9042\n",
    "# 70.2-3     1.92-2  27.8-1   \n",
    "#ThreeLayer ThreeLayerBN RNNLM aLayer abLayer\n",
    "model = ThreeLayerBN(300, 25)\n",
    "##input dim, out dim\n",
    "inp_size = [300]\n",
    "stat(model, inp_size)\n",
    "\n",
    "#stat(model, (3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LSTM_flops(module, inp, out):\n",
    "    assert isinstance(module, nn.LSTM)\n",
    "    assert len(inp.size()) == 2 and len(out.size()) == 2\n",
    "    #nn.LSTM(self.d_emb, self.d_hid, self.num_layers, batch_first=True, dropout = 0.02)\n",
    "    batch_size = inp.size()[0]\n",
    "    return batch_size * inp.size()[1] * out.size()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
